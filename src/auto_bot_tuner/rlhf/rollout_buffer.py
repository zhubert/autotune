"""
Rollout buffer for storing PPO trajectories.

During PPO training, we generate rollouts (sequences) from the policy,
score them with the reward model, and store them for training updates.
"""

import torch
from typing import Optional, List, Dict, Any
from dataclasses import dataclass


@dataclass
class RolloutBatch:
    """
    A batch of rollout data for PPO training.

    Contains all information needed to compute PPO loss:
    - Generated sequences and their log probabilities
    - Reward scores from the reward model
    - Value estimates from the value network
    - Computed advantages using GAE
    """

    # Input data
    query_tensors: torch.Tensor  # Prompts (batch_size, prompt_len)
    response_tensors: torch.Tensor  # Generated responses (batch_size, response_len)

    # Model outputs during generation
    logprobs: torch.Tensor  # Log probs of generated tokens (batch_size, response_len)
    values: torch.Tensor  # Value estimates (batch_size, response_len)

    # Rewards and advantages
    rewards: torch.Tensor  # Reward for each response (batch_size,)
    advantages: torch.Tensor  # GAE advantages (batch_size, response_len)
    returns: torch.Tensor  # Discounted returns (batch_size, response_len)

    # Masks
    attention_mask: Optional[torch.Tensor] = None  # Mask for padding

    def to(self, device: torch.device) -> "RolloutBatch":
        """Move all tensors to the specified device."""
        return RolloutBatch(
            query_tensors=self.query_tensors.to(device),
            response_tensors=self.response_tensors.to(device),
            logprobs=self.logprobs.to(device),
            values=self.values.to(device),
            rewards=self.rewards.to(device),
            advantages=self.advantages.to(device),
            returns=self.returns.to(device),
            attention_mask=self.attention_mask.to(device) if self.attention_mask is not None else None
        )

    def __len__(self) -> int:
        """Return the batch size."""
        return self.query_tensors.shape[0]


class RolloutBuffer:
    """
    Storage for PPO rollouts.

    Collects trajectories generated by the policy model, along with their
    rewards and value estimates. Used to create batches for PPO updates.

    Educational Note:
        In PPO, we alternate between:
        1. Rollout phase: Generate sequences, get rewards, compute advantages
        2. Update phase: Train policy and value function on collected rollouts

        This buffer manages the storage and retrieval of rollout data.
    """

    def __init__(
        self,
        buffer_size: int,
        device: torch.device = torch.device("cpu")
    ):
        """
        Args:
            buffer_size: Maximum number of rollouts to store
            device: Device to store tensors on
        """
        self.buffer_size = buffer_size
        self.device = device
        self.clear()

    def clear(self):
        """Clear all stored rollouts."""
        self.queries: List[torch.Tensor] = []
        self.responses: List[torch.Tensor] = []
        self.logprobs_list: List[torch.Tensor] = []
        self.values_list: List[torch.Tensor] = []
        self.rewards_list: List[torch.Tensor] = []
        self.advantages_list: List[torch.Tensor] = []
        self.returns_list: List[torch.Tensor] = []
        self.masks: List[torch.Tensor] = []
        self.size = 0

    def is_full(self) -> bool:
        """Check if buffer is full."""
        return self.size >= self.buffer_size

    def add(
        self,
        query: torch.Tensor,
        response: torch.Tensor,
        logprobs: torch.Tensor,
        values: torch.Tensor,
        reward: torch.Tensor,
        advantage: torch.Tensor,
        returns: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ):
        """
        Add a single rollout to the buffer.

        Args:
            query: Prompt tokens (prompt_len,)
            response: Generated response tokens (response_len,)
            logprobs: Log probabilities of response tokens (response_len,)
            values: Value estimates for response (response_len,)
            reward: Final reward score (scalar)
            advantage: Advantage estimates (response_len,)
            returns: Discounted returns (response_len,)
            mask: Attention mask for response (response_len,)
        """
        if self.is_full():
            raise ValueError(f"Buffer is full (size={self.buffer_size})")

        self.queries.append(query.to(self.device))
        self.responses.append(response.to(self.device))
        self.logprobs_list.append(logprobs.to(self.device))
        self.values_list.append(values.to(self.device))
        self.rewards_list.append(reward.to(self.device))
        self.advantages_list.append(advantage.to(self.device))
        self.returns_list.append(returns.to(self.device))

        if mask is not None:
            self.masks.append(mask.to(self.device))

        self.size += 1

    def get_batch(self, indices: Optional[List[int]] = None) -> RolloutBatch:
        """
        Get a batch of rollouts.

        Args:
            indices: Optional list of indices to retrieve. If None, return all.

        Returns:
            RolloutBatch containing the selected rollouts
        """
        if self.size == 0:
            raise ValueError("Buffer is empty")

        if indices is None:
            indices = list(range(self.size))

        # Pad sequences to same length for batching
        queries = [self.queries[i] for i in indices]
        responses = [self.responses[i] for i in indices]
        logprobs = [self.logprobs_list[i] for i in indices]
        values = [self.values_list[i] for i in indices]
        advantages = [self.advantages_list[i] for i in indices]
        returns = [self.returns_list[i] for i in indices]

        # Pad to max length in batch
        query_tensors = torch.nn.utils.rnn.pad_sequence(
            queries, batch_first=True, padding_value=0
        )
        response_tensors = torch.nn.utils.rnn.pad_sequence(
            responses, batch_first=True, padding_value=0
        )
        logprobs_tensors = torch.nn.utils.rnn.pad_sequence(
            logprobs, batch_first=True, padding_value=0.0
        )
        values_tensors = torch.nn.utils.rnn.pad_sequence(
            values, batch_first=True, padding_value=0.0
        )
        advantages_tensors = torch.nn.utils.rnn.pad_sequence(
            advantages, batch_first=True, padding_value=0.0
        )
        returns_tensors = torch.nn.utils.rnn.pad_sequence(
            returns, batch_first=True, padding_value=0.0
        )

        # Stack rewards (these are scalars)
        rewards_tensors = torch.stack([self.rewards_list[i] for i in indices])

        # Create attention mask
        if self.masks:
            masks = [self.masks[i] for i in indices]
            attention_mask = torch.nn.utils.rnn.pad_sequence(
                masks, batch_first=True, padding_value=0
            )
        else:
            # Create mask from response lengths
            attention_mask = (response_tensors != 0).long()

        return RolloutBatch(
            query_tensors=query_tensors,
            response_tensors=response_tensors,
            logprobs=logprobs_tensors,
            values=values_tensors,
            rewards=rewards_tensors,
            advantages=advantages_tensors,
            returns=returns_tensors,
            attention_mask=attention_mask
        )

    def get_all_batches(self, batch_size: int) -> List[RolloutBatch]:
        """
        Split buffer into multiple batches.

        Args:
            batch_size: Size of each batch

        Returns:
            List of RolloutBatch objects
        """
        batches = []
        num_batches = (self.size + batch_size - 1) // batch_size

        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, self.size)
            indices = list(range(start_idx, end_idx))
            batches.append(self.get_batch(indices))

        return batches

    def __len__(self) -> int:
        """Return the number of stored rollouts."""
        return self.size


def compute_gae(
    rewards: torch.Tensor,
    values: torch.Tensor,
    gamma: float = 0.99,
    lam: float = 0.95,
    mask: Optional[torch.Tensor] = None
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Compute Generalized Advantage Estimation (GAE).

    GAE is a method for computing advantage estimates that balances bias
    and variance. It uses a weighted combination of n-step advantages.

    Args:
        rewards: Rewards for each timestep (seq_len,) or (batch_size, seq_len)
        values: Value estimates (seq_len,) or (batch_size, seq_len)
        gamma: Discount factor (0.99 typical)
        lam: GAE lambda parameter (0.95 typical)
        mask: Optional mask for valid timesteps (seq_len,) or (batch_size, seq_len)

    Returns:
        advantages: GAE advantages (same shape as rewards)
        returns: Discounted returns (same shape as rewards)

    Educational Note:
        GAE computes advantages as:
        A_t = δ_t + (γλ)δ_{t+1} + (γλ)²δ_{t+2} + ...

        where δ_t = r_t + γV(s_{t+1}) - V(s_t) is the TD error

        This smooths the advantage estimates, reducing variance while
        maintaining low bias (when lambda is close to 1).
    """
    # Handle both 1D and 2D inputs
    if rewards.dim() == 1:
        rewards = rewards.unsqueeze(0)
        values = values.unsqueeze(0)
        if mask is not None:
            mask = mask.unsqueeze(0)
        squeeze_output = True
    else:
        squeeze_output = False

    batch_size, seq_len = rewards.shape

    # Initialize
    advantages = torch.zeros_like(rewards)
    returns = torch.zeros_like(rewards)

    # For language models, we typically get a single reward at the end
    # and 0 for intermediate steps. We need to bootstrap from the last value.

    # Compute advantages backward through time
    gae = 0
    for t in reversed(range(seq_len)):
        if t == seq_len - 1:
            # Last timestep: no next value
            next_value = 0
        else:
            next_value = values[:, t + 1]

        # TD error: δ_t = r_t + γV(s_{t+1}) - V(s_t)
        delta = rewards[:, t] + gamma * next_value - values[:, t]

        # GAE: A_t = δ_t + (γλ)A_{t+1}
        gae = delta + gamma * lam * gae
        advantages[:, t] = gae

        # Returns: R_t = A_t + V(s_t)
        returns[:, t] = advantages[:, t] + values[:, t]

        # Apply mask if provided
        if mask is not None:
            advantages[:, t] *= mask[:, t]
            returns[:, t] *= mask[:, t]

    if squeeze_output:
        advantages = advantages.squeeze(0)
        returns = returns.squeeze(0)

    return advantages, returns


def whiten_advantages(advantages: torch.Tensor, mask: Optional[torch.Tensor] = None, eps: float = 1e-8) -> torch.Tensor:
    """
    Normalize advantages to have mean 0 and std 1.

    This is a common trick in PPO to stabilize training by ensuring
    advantages are on a consistent scale.

    Args:
        advantages: Advantage estimates
        mask: Optional mask for valid timesteps
        eps: Small epsilon for numerical stability

    Returns:
        Normalized advantages
    """
    if mask is not None:
        # Only compute stats over valid (non-masked) positions
        masked_advantages = advantages * mask
        sum_advantages = masked_advantages.sum()
        count = mask.sum()
        mean = sum_advantages / (count + eps)

        variance = ((masked_advantages - mean * mask) ** 2 * mask).sum() / (count + eps)
        std = torch.sqrt(variance + eps)
    else:
        mean = advantages.mean()
        std = advantages.std() + eps

    normalized = (advantages - mean) / std

    if mask is not None:
        normalized = normalized * mask

    return normalized
