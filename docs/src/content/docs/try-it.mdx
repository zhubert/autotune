---
title: Getting Started - Try It Yourself!
description: Hands-on tutorial - Install, train, and evaluate your first fine-tuned model
---

import { Steps, Code, Aside, Tabs, TabItem } from '@astrojs/starlight/components';

## Welcome to Hands-On Post-Training!

This guide walks you through everything from installation to training your first fine-tuned language model. By the end, you'll have:

- ‚úÖ A working development environment
- ‚úÖ A GPT-2 model fine-tuned on instructions
- ‚úÖ Hands-on experience with SFT, evaluation, and generation
- ‚úÖ Understanding of how to use the toolkit for your own projects

**Time required:** 1-2 hours (including model downloads and training)

<Aside type="tip">
  **First time with LLMs?** No problem! This guide assumes no prior experience with fine-tuning. We'll explain everything step-by-step.
</Aside>

---

## 1. Installation

### Step 1.1: Install UV Package Manager

UV is a fast Python package manager that replaces pip. It's required for this project.

<Tabs>
  <TabItem label="Linux/macOS">
    ```bash
    # Install UV
    curl -LsSf https://astral.sh/uv/install.sh | sh

    # Restart your shell or run:
    source $HOME/.cargo/env

    # Verify installation
    uv --version
    ```
</TabItem>

  <TabItem label="Windows">
    ```powershell
    # Install UV using PowerShell
    powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

    # Verify installation
    uv --version
    ```
</TabItem>
</Tabs>

**Expected output:**
```
uv 0.4.x (or higher)
```

### Step 1.2: Clone the Repository

```bash
# Clone the repository
git clone https://github.com/zhubert/autotune.git
cd autotune

# Verify you're in the right place
ls -la
```

You should see directories like `src/`, `configs/`, `tests/`, etc.

### Step 1.3: Install Dependencies

<Tabs>
  <TabItem label="NVIDIA GPU or CPU">
    ```bash
    # Install all dependencies (PyTorch, transformers, etc.)
    make install

    # This takes 2-3 minutes and installs:
    # - PyTorch with CUDA support
    # - Transformers library
    # - Training utilities
    # - Development tools
    ```
</TabItem>

  <TabItem label="AMD GPU (ROCm)">
    ```bash
    # Linux only - for AMD GPUs
    make install-rocm

    # Requires ROCm 5.7+ installed on your system
    ```
</TabItem>
</Tabs>

**Expected output:**
```
‚úì Python environment created
‚úì Dependencies installed
‚úì PyTorch installed with CUDA support
```

### Step 1.4: Verify Your Setup

Let's verify everything is working correctly:

```bash
# Run the test suite (210 tests)
make test
```

<Aside type="note">
  Tests take about 2-3 minutes to run. They verify that all components work correctly on your system.
</Aside>

**Expected output:**
```
====== 210 passed in 120.45s ======
```

If tests pass, you're ready to go! üéâ

### Step 1.5: Check CUDA Availability (Optional)

Verify GPU is available for training:

```bash
# Quick Python check
uv run python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
```

**With GPU:**
```
CUDA available: True
Device: NVIDIA GeForce RTX 3090
```

**CPU only:**
```
CUDA available: False
Device: CPU
```

<Aside type="tip">
  **No GPU?** You can still complete this tutorial! Training will be slower (~30 minutes instead of ~5 minutes for the quick example), but everything will work.
</Aside>

---

## 2. Quick Start with Interactive CLI

The easiest way to get started is using the interactive CLI. It guides you through all options with helpful prompts.

### Step 2.1: Launch Interactive Mode

```bash
python main.py
```

**You'll see:**
```
========================================
    LLM Post-Training Toolkit
========================================

Available training methods:
  1. Supervised Fine-Tuning (SFT)
  2. Reward Model Training
  3. Direct Preference Optimization (DPO)
  4. RLHF with PPO

Select a method (1-4):
```

### Step 2.2: Navigate the Menu

The interactive CLI walks you through:

1. **Choose training method** - Start with SFT (option 1)
2. **Select model** - Choose from GPT-2, Llama, or custom path
3. **Choose dataset** - Select Alpaca, Dolly, or provide your own
4. **Configure training** - Set batch size, learning rate, epochs
5. **Start training** - Monitor progress in real-time

<Aside type="tip">
  The interactive CLI is perfect for exploring! You can see all options without memorizing command-line flags.
</Aside>

### Step 2.3: Try a Quick Training Run

Let's do a quick 5-minute training run through the interactive menu:

1. Run `python main.py`
2. Select `1` (Supervised Fine-Tuning)
3. Choose `gpt2` as the model
4. Choose `yahma/alpaca-cleaned` as the dataset
5. When asked for preset, choose `quick` (500 samples, 1 epoch)
6. Confirm and start training

**You'll see:**
```
üöÄ Starting SFT training...
üìä Model: gpt2 (124M parameters)
üìö Dataset: yahma/alpaca-cleaned (500 samples)
üéØ Using LoRA for efficient training

Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [03:45<00:00]
  Loss: 2.34 ‚Üí 1.89
  Perplexity: 10.38 ‚Üí 6.62

‚úÖ Training complete!
üíæ Model saved to: checkpoints/sft/quick/
```

Congratulations! You've trained your first model! üéâ

---

## 3. Training Your First SFT Model (Complete Example)

Now let's do a more thorough training run using the command-line interface. This gives you more control and is better for reproducible experiments.

### Step 3.1: Understanding the Training Config

First, let's look at a complete training configuration:

```bash
# View the quick SFT config
cat configs/sft_gpt2_quick.yaml
```

**You'll see:**
```yaml
# Quick SFT Training Configuration
model:
  name: "gpt2"
  use_lora: true          # Efficient training with LoRA

dataset:
  name: "yahma/alpaca-cleaned"
  max_samples: 500        # Subset for quick testing
  max_length: 256         # Maximum sequence length
  format_type: "alpaca"   # Alpaca dataset format

training:
  learning_rate: 2e-5
  batch_size: 4
  gradient_accumulation_steps: 4  # Effective batch size = 16
  num_epochs: 1
  warmup_steps: 50
  max_grad_norm: 1.0
  logging_steps: 10
  save_steps: 500
  bf16: true              # Use BF16 for faster training
  output_dir: "checkpoints/sft/quick"
```

### Step 3.2: Train with Config File

The cleanest way to train is using a config file:

```bash
# Train using the quick config
python main.py train-config configs/sft_gpt2_quick.yaml
```

<Aside type="note">
  **Training time:**
  - With GPU (RTX 3090): ~3-5 minutes
  - With CPU: ~20-30 minutes
</Aside>

**Expected output:**
```bash
üîß Loading configuration from configs/sft_gpt2_quick.yaml
üì• Loading model: gpt2
   ‚Ä¢ Using LoRA adapters (trainable params: 294,912 / 124M = 0.24%)
   ‚Ä¢ Device: cuda
   ‚Ä¢ Precision: bf16

üìö Loading dataset: yahma/alpaca-cleaned
   ‚Ä¢ Format: alpaca (instruction, input, output)
   ‚Ä¢ Samples: 500
   ‚Ä¢ Max length: 256 tokens
   ‚Ä¢ Train/val split: 450/50

üöÄ Starting training...

Epoch 1/1:
  Step   10/125 | Loss: 2.847 | LR: 4.00e-06 | Tokens/sec: 1243
  Step   20/125 | Loss: 2.654 | LR: 8.00e-06 | Tokens/sec: 1289
  Step   30/125 | Loss: 2.423 | LR: 1.20e-05 | Tokens/sec: 1301
  ...
  Step  120/125 | Loss: 1.892 | LR: 2.00e-05 | Tokens/sec: 1324

üìä Training Summary:
   ‚Ä¢ Final Loss: 1.892
   ‚Ä¢ Final Perplexity: 6.63
   ‚Ä¢ Total time: 4m 23s
   ‚Ä¢ Tokens processed: 512,000

üíæ Model saved to: checkpoints/sft/quick/
   ‚Ä¢ Final checkpoint: checkpoints/sft/quick/checkpoint-final/
   ‚Ä¢ LoRA adapters: checkpoints/sft/quick/checkpoint-final/adapter_model.bin

‚úÖ Training complete!
```

### Step 3.3: Train with CLI Arguments

Alternatively, you can train using command-line arguments:

```bash
# Same training, but with CLI arguments
python main.py train-sft \
  --model gpt2 \
  --dataset yahma/alpaca-cleaned \
  --preset quick \
  --batch-size 4 \
  --grad-accum 4 \
  --learning-rate 2e-5 \
  --epochs 1 \
  --max-samples 500
```

<Aside type="tip">
  **Use configs for reproducibility!** Config files are easier to version control and share with collaborators.
</Aside>

### Step 3.4: Understanding What Happened

During training, several things occurred:

1. **Model Loading**
   - Downloaded GPT-2 (124M parameters) from Hugging Face
   - Added LoRA adapters (only 0.24% of params are trainable)
   - Moved to GPU if available

2. **Dataset Preparation**
   - Downloaded Alpaca dataset (52K instruction-response pairs)
   - Selected 500 samples for quick training
   - Formatted as: `Below is an instruction... ### Instruction: ... ### Response: ...`
   - Tokenized and masked prompts (loss only on responses)

3. **Training Loop**
   - 125 steps (500 samples √∑ 4 batch size)
   - Gradient accumulation simulates batch size of 16
   - Loss decreased from ~2.8 to ~1.9
   - Perplexity decreased from ~16 to ~6.6

4. **Model Saved**
   - LoRA adapters saved (only ~1.2MB!)
   - Can be loaded later for inference or continued training

---

## 4. Evaluating Your Model

Now let's test the model and see if it actually learned to follow instructions!

### Step 4.1: Generate Text from Your Model

Create a simple test script:

```python
# test_model.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

def load_trained_model(checkpoint_path):
    """Load base model + LoRA adapters"""
    print("Loading model...")

    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained("gpt2")

    # Load LoRA adapters
    model = PeftModel.from_pretrained(base_model, checkpoint_path)
    model.eval()

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)

    return model, tokenizer, device

def generate_response(model, tokenizer, device, instruction, max_length=100):
    """Generate a response to an instruction"""
    # Format instruction in Alpaca format
    prompt = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n"

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # Decode and extract response
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = full_text.split("### Response:\n")[-1].strip()

    return response

# Load model
model, tokenizer, device = load_trained_model("checkpoints/sft/quick/checkpoint-final")

# Test instructions
test_instructions = [
    "Write a haiku about machine learning.",
    "Explain what a neural network is in one sentence.",
    "List three benefits of exercise.",
    "What is the capital of France?",
]

print("\n" + "="*60)
print("Testing Fine-Tuned Model")
print("="*60 + "\n")

for instruction in test_instructions:
    print(f"üìù Instruction: {instruction}")
    response = generate_response(model, tokenizer, device, instruction)
    print(f"ü§ñ Response: {response}")
    print("-" * 60 + "\n")
```

Run it:

```bash
uv run python test_model.py
```

**Expected output:**
```
Loading model...

============================================================
Testing Fine-Tuned Model
============================================================

üìù Instruction: Write a haiku about machine learning.
ü§ñ Response: Data patterns flow,
Algorithms learn and grow,
Knowledge from the code.
------------------------------------------------------------

üìù Instruction: Explain what a neural network is in one sentence.
ü§ñ Response: A neural network is a computational model inspired by
biological neurons that learns to recognize patterns in data through
layers of interconnected nodes.
------------------------------------------------------------

üìù Instruction: List three benefits of exercise.
ü§ñ Response: 1. Improves cardiovascular health
2. Increases energy levels
3. Reduces stress and anxiety
------------------------------------------------------------

üìù Instruction: What is the capital of France?
ü§ñ Response: The capital of France is Paris.
------------------------------------------------------------
```

### Step 4.2: Compare with Base Model

Let's compare the fine-tuned model with the base model:

```python
# compare_models.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

def generate_base_response(instruction):
    """Generate response from base (non-fine-tuned) GPT-2"""
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)

    prompt = f"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n"

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=100,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test comparison
instruction = "Write a haiku about machine learning."

print("\n" + "="*60)
print("Base Model vs Fine-Tuned Model")
print("="*60 + "\n")

print(f"üìù Instruction: {instruction}\n")

print("üî¥ Base GPT-2 (not fine-tuned):")
base_response = generate_base_response(instruction)
print(base_response)
print("\n" + "-"*60 + "\n")

print("üü¢ Fine-Tuned GPT-2:")
# (Load fine-tuned model and generate as in previous example)
print("Data patterns flow,\nAlgorithms learn and grow,\nKnowledge from the code.")
print("\n" + "="*60)
```

**You'll see the difference:**
```
Base Model (üî¥): Continues the text pattern, doesn't follow instruction
Fine-Tuned (üü¢): Generates appropriate haiku following the instruction
```

### Step 4.3: Quantitative Evaluation

The toolkit includes built-in evaluation metrics:

```bash
# Evaluate on validation set
python main.py evaluate \
  --checkpoint checkpoints/sft/quick/checkpoint-final \
  --dataset yahma/alpaca-cleaned \
  --split validation \
  --num-samples 100
```

**Expected metrics:**
```
üìä Evaluation Results:
   ‚Ä¢ Perplexity: 6.63 (lower is better)
   ‚Ä¢ Loss: 1.892
   ‚Ä¢ Tokens/sec: 2341

üìà Comparison to base model:
   ‚Ä¢ Base perplexity: 45.2
   ‚Ä¢ Fine-tuned perplexity: 6.63
   ‚Ä¢ Improvement: 85% reduction
```

<Aside type="tip">
  **What do these metrics mean?**
  - **Perplexity**: How "surprised" the model is by the text. Lower = better prediction.
  - **Loss**: Training objective value. Lower = better fit to training data.
  - A perplexity drop from 45 ‚Üí 6.6 indicates the model learned the pattern well!
</Aside>

---

## 5. Next Steps - Advanced Training

Congratulations! You've trained your first SFT model. Here's what to explore next:

### Step 5.1: Train a Reward Model

Reward models learn to predict which responses humans prefer:

```bash
# Train a reward model on preference data
python main.py train-config configs/reward_model.yaml
```

**What this does:**
- Uses preference data (prompt, chosen_response, rejected_response)
- Trains a model to score responses
- Essential for RLHF training

**Time:** ~30 minutes with GPU

See the [Reward Modeling guide](/reward/) for details.

### Step 5.2: Try Direct Preference Optimization (DPO)

DPO aligns models with preferences without needing a separate reward model:

```bash
# Train with DPO
python main.py train-config configs/dpo_standard.yaml
```

**What this does:**
- Takes your SFT model as starting point
- Optimizes directly on preference pairs
- Simpler and more stable than RLHF

**Time:** ~1 hour with GPU

See the [DPO guide](/dpo/) for details.

### Step 5.3: Try Full RLHF with PPO

The classic approach used by ChatGPT and Claude:

```bash
# Train with RLHF
python main.py train-rlhf \
  --policy-model checkpoints/sft/quick/checkpoint-final \
  --reward-model checkpoints/reward/checkpoint-final \
  --dataset Anthropic/hh-rlhf \
  --num-rollouts 1000
```

**What this does:**
- Uses your SFT model as policy
- Uses reward model to score outputs
- Optimizes with PPO reinforcement learning
- Adds KL penalty to prevent drift

**Time:** ~2 hours with GPU

See the [RLHF guide](/rlhf/) for details.

### Step 5.4: Try Larger Models

Once comfortable with GPT-2, try larger models:

```bash
# Train Llama 3.2 1B with LoRA (requires ~8GB VRAM)
python main.py train-sft \
  --model meta-llama/Llama-3.2-1B \
  --dataset yahma/alpaca-cleaned \
  --preset standard \
  --batch-size 2 \
  --grad-accum 8
```

<Aside type="caution">
  **Note:** You'll need a Hugging Face account and to accept the Llama license before downloading Llama models.
</Aside>

### Step 5.5: Create Custom Datasets

Use your own instruction data:

```python
# custom_dataset.py
from datasets import Dataset

# Your custom data
data = [
    {
        "instruction": "Your instruction here",
        "input": "",  # Optional context
        "output": "Expected response"
    },
    # ... more examples
]

# Create dataset
dataset = Dataset.from_list(data)
dataset.push_to_hub("your-username/your-dataset")

# Train on it
# python main.py train-sft --dataset your-username/your-dataset --preset standard
```

---

## 6. Using as a Library

Beyond the CLI, you can use this toolkit as a Python library for custom experiments.

### Step 6.1: Basic SFT Training

```python
from src.auto_bot_tuner.sft import SFTTrainer, SFTConfig, InstructionDataset
from src.auto_bot_tuner.utils.model_loading import load_model_and_tokenizer
from datasets import load_dataset

# Load model (automatically detects GPU, sets up LoRA)
model, tokenizer, device = load_model_and_tokenizer(
    "gpt2",
    use_lora=True
)

# Load dataset
raw_dataset = load_dataset("yahma/alpaca-cleaned", split="train[:1000]")
dataset = InstructionDataset(
    raw_dataset,
    tokenizer,
    max_length=256,
    format_type="alpaca"
)

# Configure training
config = SFTConfig(
    learning_rate=2e-5,
    batch_size=4,
    num_epochs=1,
    output_dir="my_experiment"
)

# Train
trainer = SFTTrainer(model, tokenizer, dataset, config=config)
trainer.train()

# Save
trainer.save_model("my_experiment/final")
```

### Step 6.2: Custom Loss Functions

Create custom training objectives:

```python
from src.auto_bot_tuner.sft.loss import compute_sft_loss
import torch
import torch.nn.functional as F

def custom_sft_loss(logits, labels, alpha=0.1):
    """
    Custom SFT loss with length penalty.

    Encourages shorter, more concise responses.
    """
    # Standard cross-entropy loss
    ce_loss, metrics = compute_sft_loss(logits, labels)

    # Add length penalty
    response_length = (labels != -100).sum()
    length_penalty = alpha * response_length

    total_loss = ce_loss + length_penalty

    metrics["length_penalty"] = length_penalty.item()

    return total_loss, metrics

# Use in training
# (Modify trainer to use custom loss function)
```

### Step 6.3: Custom Evaluation Metrics

Implement custom evaluation:

```python
from src.auto_bot_tuner.evaluation import evaluate_generation_quality
import torch

def evaluate_model_on_tasks(model, tokenizer, tasks):
    """
    Evaluate model on specific tasks.

    Args:
        model: Fine-tuned model
        tokenizer: Tokenizer
        tasks: List of (instruction, expected_keywords) tuples

    Returns:
        dict: Evaluation metrics
    """
    results = []

    for instruction, expected_keywords in tasks:
        # Generate response
        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"
        inputs = tokenizer(prompt, return_tensors="pt")

        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=100)

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Check if expected keywords appear
        keywords_found = sum(1 for kw in expected_keywords if kw.lower() in response.lower())
        score = keywords_found / len(expected_keywords)

        results.append(score)

    return {
        "avg_keyword_accuracy": sum(results) / len(results),
        "perfect_matches": sum(1 for s in results if s == 1.0) / len(results)
    }

# Example usage
tasks = [
    ("What is the capital of France?", ["Paris"]),
    ("List three primary colors", ["red", "blue", "yellow"]),
]

metrics = evaluate_model_on_tasks(model, tokenizer, tasks)
print(f"Keyword accuracy: {metrics['avg_keyword_accuracy']:.2%}")
```

### Step 6.4: Batch Inference

Process multiple prompts efficiently:

```python
from src.auto_bot_tuner.evaluation.generation import generate_batch
import torch

def batch_generate_responses(model, tokenizer, instructions, batch_size=8):
    """
    Generate responses for multiple instructions efficiently.

    Args:
        model: Fine-tuned model
        tokenizer: Tokenizer
        instructions: List of instruction strings
        batch_size: Number of instructions to process at once

    Returns:
        List of generated responses
    """
    responses = []
    device = next(model.parameters()).device

    for i in range(0, len(instructions), batch_size):
        batch = instructions[i:i+batch_size]

        # Format prompts
        prompts = [
            f"### Instruction:\n{inst}\n\n### Response:\n"
            for inst in batch
        ]

        # Tokenize with padding
        inputs = tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256
        ).to(device)

        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decode
        batch_responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        responses.extend([r.split("### Response:\n")[-1].strip() for r in batch_responses])

    return responses

# Example
instructions = [
    "What is Python?",
    "How do neural networks work?",
    "Explain photosynthesis briefly.",
    # ... many more
]

responses = batch_generate_responses(model, tokenizer, instructions)
for inst, resp in zip(instructions, responses):
    print(f"Q: {inst}\nA: {resp}\n")
```

### Step 6.5: Integration with Other Tools

Combine with popular tools:

```python
# Integration with Weights & Biases
import wandb
from src.auto_bot_tuner.sft import SFTTrainer

# Initialize W&B
wandb.init(project="my-llm-tuning", name="gpt2-alpaca-experiment")

# Custom trainer with W&B logging
class WandBSFTTrainer(SFTTrainer):
    def log_metrics(self, metrics, step):
        """Override to log to W&B"""
        super().log_metrics(metrics, step)
        wandb.log(metrics, step=step)

# Train with W&B logging
trainer = WandBSFTTrainer(model, tokenizer, dataset, config)
trainer.train()

# Log final model
wandb.save("checkpoints/model.bin")
```

---

## 7. Troubleshooting

Common issues and solutions:

### Issue: Out of Memory (OOM)

**Symptom:**
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**Solutions:**

```bash
# 1. Reduce batch size
python main.py train-sft --model gpt2 --dataset yahma/alpaca-cleaned \
  --batch-size 2  # Instead of 4
  --grad-accum 8  # Maintain effective batch size

# 2. Reduce sequence length
# Edit config:
dataset:
  max_length: 128  # Instead of 256

# 3. Use gradient checkpointing (edit code)
model.gradient_checkpointing_enable()

# 4. Use smaller model
python main.py train-sft --model gpt2  # Instead of gpt2-medium
```

### Issue: Slow Training on CPU

**Symptom:**
Training takes hours instead of minutes.

**Solutions:**

```bash
# 1. Use smaller subset for testing
python main.py train-sft --model gpt2 --dataset yahma/alpaca-cleaned \
  --max-samples 100  # Very small subset

# 2. Reduce epochs
python main.py train-sft --model gpt2 --dataset yahma/alpaca-cleaned \
  --epochs 1  # Just one pass

# 3. Consider Google Colab
# Free GPU access for small projects
# Upload your code and run there
```

### Issue: Model Not Following Instructions

**Symptom:**
After training, model still doesn't follow instructions well.

**Solutions:**

```bash
# 1. Train longer
--epochs 3  # Instead of 1

# 2. Use more data
--max-samples 5000  # Instead of 500

# 3. Verify dataset format
# Check that dataset is properly formatted:
uv run python -c "
from datasets import load_dataset
ds = load_dataset('yahma/alpaca-cleaned', split='train[:5]')
print(ds[0])
"

# 4. Check loss is decreasing
# Loss should go from ~3.0 to ~1.5-2.0
# If not decreasing, adjust learning rate
```

### Issue: Import Errors

**Symptom:**
```
ModuleNotFoundError: No module named 'transformers'
```

**Solutions:**

```bash
# 1. Reinstall dependencies
make install

# 2. Use uv run for scripts
uv run python test_model.py  # Instead of: python test_model.py

# 3. Activate environment manually
source .venv/bin/activate
python test_model.py
```

### Issue: Dataset Download Fails

**Symptom:**
```
ConnectionError: Couldn't reach the Hugging Face Hub
```

**Solutions:**

```bash
# 1. Check internet connection
curl https://huggingface.co

# 2. Set cache directory
export HF_HOME=/path/with/more/space

# 3. Download manually
from datasets import load_dataset
ds = load_dataset("yahma/alpaca-cleaned", cache_dir="./my_cache")

# 4. Use offline mode (if already downloaded)
export HF_DATASETS_OFFLINE=1
```

### Issue: LoRA Adapters Not Loading

**Symptom:**
```
ValueError: Cannot load adapter model from checkpoint
```

**Solutions:**

```python
# Verify checkpoint structure
import os
checkpoint_path = "checkpoints/sft/quick/checkpoint-final"

print("Files in checkpoint:")
for file in os.listdir(checkpoint_path):
    print(f"  - {file}")

# Should see:
#   - adapter_config.json
#   - adapter_model.bin

# Load correctly:
from peft import PeftModel
from transformers import AutoModelForCausalLM

base_model = AutoModelForCausalLM.from_pretrained("gpt2")
model = PeftModel.from_pretrained(base_model, checkpoint_path)
```

### Issue: CUDA Version Mismatch

**Symptom:**
```
RuntimeError: CUDA error: no kernel image is available for execution
```

**Solutions:**

```bash
# 1. Check CUDA version
nvidia-smi

# 2. Reinstall PyTorch for your CUDA version
# Visit: https://pytorch.org/get-started/locally/

# For CUDA 11.8:
uv pip install torch --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1:
uv pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### Still Having Issues?

1. **Check the logs**: Training logs are saved to `logs/` directory
2. **Run tests**: `make test` to verify installation
3. **Open an issue**: [GitHub Issues](https://github.com/zhubert/autotune/issues)
4. **Check discussions**: [GitHub Discussions](https://github.com/zhubert/autotune/discussions)

---

## What's Next?

You've completed the getting started guide! Here's your learning path forward:

### Deepen Your Understanding

1. **[SFT Deep Dive](/sft/)** - Learn the mathematics and implementation details
2. **[Reward Modeling](/reward/)** - Understand how to train preference models
3. **[DPO Guide](/dpo/)** - Modern preference optimization technique
4. **[RLHF Guide](/rlhf/)** - Classic reinforcement learning approach

### Practical Projects

1. **Fine-tune for your domain** - Use your own instruction data
2. **Compare SFT vs DPO vs RLHF** - Run all three and compare results
3. **Build a chatbot** - Deploy your fine-tuned model with a web interface
4. **Contribute improvements** - Add features and submit pull requests

### Advanced Topics

1. **Multi-task learning** - Train on diverse tasks simultaneously
2. **Constitutional AI** - Use AI-generated feedback for alignment
3. **Scaling up** - Train larger models with distributed training
4. **Production deployment** - Serve models with optimized inference

<Aside type="tip">
  **Join the community!** Share your results, ask questions, and help others learn. Post in [GitHub Discussions](https://github.com/zhubert/autotune/discussions).
</Aside>

---

## Summary

In this guide, you:

‚úÖ Installed UV and all dependencies
‚úÖ Verified your setup with 210 passing tests
‚úÖ Explored the interactive CLI
‚úÖ Trained your first SFT model on GPT-2 + Alpaca
‚úÖ Evaluated and compared with the base model
‚úÖ Learned about DPO, RLHF, and reward modeling
‚úÖ Saw how to use the toolkit as a Python library
‚úÖ Troubleshot common issues

**You're now ready to fine-tune language models!** üéâ

Continue your journey by diving deep into each technique:
- üìö [Supervised Fine-Tuning Details](/sft/)
- ‚≠ê [Reward Modeling Guide](/reward/)
- üéØ [Direct Preference Optimization](/dpo/)
- üöÄ [RLHF with PPO](/rlhf/)

Happy fine-tuning! ü§ñ
